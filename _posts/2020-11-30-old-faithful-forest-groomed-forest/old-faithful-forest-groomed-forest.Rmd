---
title: "Old faithful, Meet the Neighbors, and Lost in the Forest"
description: |
  Model description and evaluation.
author:
  - name: Shaina Trevino, Jonathan Pedroza, Akhila Nekkanti
    url: https://github.com/akhilanekkanti1/Final_ML_Blog
date: 11-30-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
library(tidyverse)
library(tidymodels)
```

# Overview

In this post we are going to describe our process for the three machine learning models that we ran (e.g., penalized regression, k-nearest neighbor, random forest) and compare the performance of all models. The results that are presented are from the full dataset described in the previous post. Since the full dataset was so large, we utilized a High Performance Computing (HPC) cluster from the University of Oregon (Talapas) to run all models. 

## Split the Data 

Before we start fitting models, the first step is to split our data into two parts: training and testing data. To do this, we use `initial_split` to split the data and then define our training and testing set. Notice that we also used stratified sampling (`strata = "score"`) when splitting our data to ensure the training and testing sets have similar outcome responses.

```{r splits, echo=TRUE}
set.seed(1272020)

d_split <- initial_split(d, strata = "score")

d_train <- training(d_split)
d_test  <- testing(d_split)
```

Then, we take our testing set and create a k-fold cross validation object. This splits our training set into 10 different samples of data each with their own training and testing set. We will use this object to evaluate the performance metrics for each model across all 10 samples.

```{r cv, echo=TRUE}
set.seed(1272020)

train_cv <- vfold_cv(d_train, strata = "score")
```


## Define the Recipe 

This recipe is explained in the previous post. However, we provided the code here as well for completeness. We will use this same recipe for all models. 

```{r recipe-for-all}
rec_yoself <- recipe(score ~ .,data = d_train) %>%
  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt))) %>% 
  update_role(contains("id"), ncessch, new_role = "id vars") %>%
  step_unknown(all_nominal()) %>% 
  step_novel(all_nominal()) %>% 
  step_dummy(all_nominal()) %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%  
  step_interact(terms = ~lat:lon) %>% 
  step_nzv(all_predictors())
```


# Model 1: Old Faithful - Penalized Regression Model

Finally, we are ready to create our first model. This will be a penalized regression, specifically an elastic net model. Linear regression models are usually easier to interpret and less computationally intensive. 

To run an enet model, you must specify the penalty argument (for dealing with multicolinearity) and the mixture argument (proportion of ridge [0] to lasso [1] models). For our purposes, we will tune these parameters to find the values that result in the best model performance. 

```{r lrmod}
lr_mod <- linear_reg()  %>%
  set_engine("glmnet") %>% 
  set_mode("regression") %>% 
  set_args(penalty = tune(),
           mixture = tune())

```

### Workflow

After specifying our enet model, we use the `workflow` package to combine our recipe and our model into one workflow object. This makes it easier to monitor and update your recipe and model that you are working with.

```{r lr-flo}
lr_flo <- workflow() %>% 
  add_recipe(rec_yoself) %>% 
  add_model(lr_mod)

```

#### Tune Model

We are now ready to tune our enet model. To tune the model we first need to create a grid for tuning parameters. Since we are using a linear regression model, we decided to use a regular grid with our two hyperparameters (`penalty()` and `mixture()`). We also specified 30 levels (i.e., 30 values for each hyperparameter) to create the grid.

```{r tune-lr}
#create grid
lr_grd <- grid_regular(penalty(), mixture(), levels = 30)

#tune model 
tictoc::tic()
lr_res <- tune::tune_grid(lr_flo, resamples = train_cv, grid = lr_grd,
                           control = tune::control_resamples(save_pred = TRUE))
tictoc::toc()

```

When we ran the previous model on the HPC cluster, it only took 26 minutes to tune. The values that lead to the highest performance (i.e., lowest RMSE) for our tuning parameters were: 

+ Penalty = <0.00

+ Mixture =  0.03

Note: Because both values are so low, we would expect to get very similar results with a ridge regression model and it would be less computationally intensive. 

#### Apply Best Tuning Parameters

We can then apply these values to our arguments in our model with the select `select_best` function. Remember to update and finalize your workflow with the new model as well. 

```{r best-lr}
#select best tuning parameters
lr_best <- lr_res %>% 
  select_best(metric = "rmse")

#finalize model in workflow
lr_flo_final <- finalize_workflow(lr_flo, lr_best)

```

### Model 1 Results

Once we have finalized our model and workflow, we can use the `last_fit()` function to apply our recipe and model to the `initial_split` object we made. This will evaluate the model performance on the testing set. We then use the `collect_metrics()` function to view our performance metrics on the test set.

```{r lr-fit}
#evaluate on test set with last_fit
lr_final_res <- last_fit(
  lr_flo_final,
  split = d_split)

#view performance metric
lr_final_res %>%
  collect_metrics() %>% 
  filter(`.metric` == "rmse")

```

```{r import-Rds, eval = TRUE, echo = FALSE}
lr_rds <- readRDS("model1-lr.Rds") #final fit object

lr_rds %>%  
  collect_metrics() %>% 
  filter(`.metric` == "rmse")

# wrkfl %>% 
#   pull_workflow_spec()
# 
# wrkfl <- workflow(wrkfl)
# 
# lr_rds$.workflow
# #$fit$spec$args
```

As you can see, our final performance metric (RMSE) for our penalized regression model is `89.03`. 

#### Get Predictions

We also wanted to note that if you wanted to extract predictions from the test dataset within the split object (e.g., `d_split`), you can use the `collect_predictions()` function. However, our goal is to compare this model performance with two different models and then extract predictions from the best one. 

```{r lr-pred}
test_preds <- lr_final_res %>% collect_predictions()
test_preds
```

# Model 2: Meet the Neighbors - K Nearest Neighbor (KNN) (AN)

We'll use the same data and recipe from our first Model. 

```{r}
knn_mod <- nearest_neighbor()  %>%
  set_engine("kknn") %>% 
  set_mode("regression") %>% 
  set_args(neighbors = tune())

#had to take out other tuning parameters due to run times > 16 hours on talapas (e.g., weight_func = tune(), dist_power = tune()))
```

Create a workflow
```{r}

knn_flo <- workflow() %>% 
  add_recipe(rec_yoself) %>% 
  add_model(knn_mod)

```

Set grid
```{r}
#had to take out due to computation
#knn_par <- parameters(neighbors(range = (c(10, 75))), weight_func(), dist_power()) #testing with smaller range due to computation
knn_grd <- grid_max_entropy(neighbors(), size = 30) #testing with smaller size due to computation 
```

Tune grid
```{r}
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
foreach::getDoParWorkers()
clusterEvalQ(cl, {library(tidymodels)})

tictoc::tic()
knn_res <- tune::tune_grid(knn_flo, resamples = train_cv, grid = knn_grd,
                           control = tune::control_resamples(save_pred = TRUE))
parallel::stopCluster(cl)
tictoc::toc()

#1772.517 sec elapsed - from talapas - over 16 hours with all tuning parameters
```

Select the best tuning parameters and use them to finalize the workflow.

+ Final parameters chosen (after tuning/selecting best) - 10 neighbors (others are default due to computation time)
```{r}
knn_best <- knn_res %>% 
  select_best(metric = "rmse")

knn_flo_final <- finalize_workflow(knn_flo, knn_best)
```


```{r}
registerDoSEQ() 
knn_final_res <- last_fit(
  knn_flo_final,
  split = d_split)

knn_final_res %>%
  collect_metrics()

```


# Model 3: Lost in the Forest - Random Forest (JP)]

Final parameters chosen (after tuning/selecting best) - mtry = 5, min_n = 40, trees = 1000

13919.058 sec elapsed - from talapas

### Modified recipe to increase predictive performance (JP)

# Comparison

+ Plot comparing performance metrics

+ Explanation of best model

```{r from-rds-files}
#info extracted from rds files & from talapas .Rout files (see lab 4 repo/files for final folder)
#Regression model
#1552.898 sec elapsed on talapas
#RMSE = 89.02

#KNN model
#1772.517 sec elapsed on talapas
#RMSE 92.2 

#RF model
#13919.058 sec elapsed on talapas
#RMSE 85.8
```


+ How to get predictions from test.csv 

```{r predict-new-test-data}
#import
test <- read_csv("data/test.csv",
                 col_types = cols(.default = col_guess(), 
                                  calc_admn_cd = col_character()))

#join
test1 <- test %>% 
  left_join(frl_stu) %>% 
  left_join(staff) %>% 
  left_join(ethnicities)

```

```{r predict-new-test-data-final-fit}
#update with final chosen model (best fit)
#If you want to use your model to predict the response for new observations, you need to use the fit() function on your workflow and the dataset that you want to fit the final model on (e.g. the complete training + testing dataset). 

#workflow
fit_workflow <- fit(FINALIZED_WORKFLOW_OBJECT_NAME, d)#Should this be d_train based on lab 3key. This blog says not - http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/

fit_workflow #view

#use model to make predictions for test dataset (adding test1 as new data)
preds_final <- predict(fit_workflow, test1)

######################
pred_frame <- tibble(Id = test1$id, Predicted = preds_final$.pred)

#write_csv(pred_frame, "FILE-NAME.csv")

```


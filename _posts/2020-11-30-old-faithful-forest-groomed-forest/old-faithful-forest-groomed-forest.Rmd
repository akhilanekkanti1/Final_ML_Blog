---
title: "Old faithful, Meet the Neighbors, and Lost in the Forest"
description: |
  Model description and evaluation.
author:
  - name: Shaina Trevino, Jonathan Pedroza, Akhila Nekkanti
    url: https://github.com/akhilanekkanti1/Final_ML_Blog
date: 11-30-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
library(tidyverse)
library(tidymodels)
```

# Overview

+ Models were all ran with 1% of training data 

+ Run 3 different models and compare 

# Model 1: Old Faithful - Penalized Regression (ST)

### Splitting Data (same for all models)

+ splits and cv object

```{r splits, echo=TRUE}
set.seed(1272020)

d_split <- initial_split(d, strata = "score")

d_train <- training(d_split)
d_test  <- testing(d_split)
train_cv <- vfold_cv(d_train, strata = "score")
```


### Recipe (same for all models)

Note: described in data description (link to post), code is copied here for completeness/reproducibility 

```{r recipe-for-all}
rec_yoself <- recipe(score ~ .,data = d_train) %>%
  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt))) %>% #had  to add as.numeric to recipe to make xgboost model run
  update_role(contains("id"), ncessch, new_role = "id vars") %>%
  step_unknown(all_nominal()) %>% 
  step_novel(all_nominal()) %>% 
  step_dummy(all_nominal()) %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%  
  step_interact(terms = ~lat:lon) %>% 
  step_nzv(all_predictors()) #added due to error in xg boost about constant variables with 0sd
```


### Penalized Regression Model

```{r lrmod}
lr_mod <- linear_reg()  %>%
  set_engine("glmnet") %>% 
  set_mode("regression") %>% 
  set_args(penalty = tune(), #take out for tutorial, add in next step
           mixture = tune())

```


### Tuning Hyperparameters

```{r lr-mod}
lr_mod <- linear_reg()  %>%
  set_engine("glmnet") %>% 
  set_mode("regression") %>% 
  set_args(penalty = tune(), #tune
           mixture = tune())

```

### Model Results

```{r import-Rds, eval = TRUE}
lr_rds <- readRDS("model1-lr.Rds")

lr_rds %>%  
  collect_metrics() %>% 
  filter(`.metric` == "rmse")

lr_rds %>%  
  collect_metrics()

lr_rds %>%  
  show_best(metric = "rmse")

# wrkfl %>% 
#   pull_workflow_spec()
# 
# wrkfl <- workflow(wrkfl)
# 
# lr_rds$.workflow
# #$fit$spec$args
```


#### Workflow

```{r lr-flo}
lr_flo <- workflow() %>% 
  add_recipe(rec_yoself) %>% 
  add_model(lr_mod)

```

#### Tune Model

best tuning results from model tuning on Talapas: Penalty = 0.0000000001, Mixture = 0.0345
Took 1,552.90 seconds to tune on talapas

```{r tune-lr}
#grid
lr_grd <- grid_regular(penalty(), mixture(), levels = 30)

#tune model 

tictoc::tic()
lr_res <- tune::tune_grid(lr_flo, resamples = train_cv, grid = lr_grd,
                           control = tune::control_resamples(save_pred = TRUE))
tictoc::toc()

```

#### Apply Best Tuning Parameters

```{r best-lr}
#select best tuning parameters
lr_best <- lr_res %>% 
  select_best(metric = "rmse")

lr_best

#finalize model in workflow
lr_flo_final <- finalize_workflow(lr_flo, lr_best)

```

#### Final Fit

#### This will automatically train the model specified by the workflow using the training data, and produce evaluations based on the test set.

```{r lr-fit}
#evaluate on test set with last_fit
lr_final_res <- last_fit(
  lr_flo_final,
  split = d_split)

lr_final_res %>%
  collect_metrics()
```

#### Get Predictions

```{r lr-pred}
#get predictions from test set (within split object)
test_preds <- lr_final_res %>% collect_predictions()
test_preds

```


# Model 2: Meet the Neighbors - K Nearest Neighbor (KNN) (AN)

We'll use the same data and recipe from our first Model. 

```{r}
knn_mod <- nearest_neighbor()  %>%
  set_engine("kknn") %>% 
  set_mode("regression") %>% 
  set_args(neighbors = tune())

#had to take out other tuning parameters due to run times > 16 hours on talapas (e.g., weight_func = tune(), dist_power = tune()))
```

Create a workflow
```{r}

knn_flo <- workflow() %>% 
  add_recipe(rec_yoself) %>% 
  add_model(knn_mod)

```

Set grid
```{r}
#had to take out due to computation
#knn_par <- parameters(neighbors(range = (c(10, 75))), weight_func(), dist_power()) #testing with smaller range due to computation
knn_grd <- grid_max_entropy(neighbors(), size = 30) #testing with smaller size due to computation 
```

Tune grid
```{r}
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
foreach::getDoParWorkers()
clusterEvalQ(cl, {library(tidymodels)})

tictoc::tic()
knn_res <- tune::tune_grid(knn_flo, resamples = train_cv, grid = knn_grd,
                           control = tune::control_resamples(save_pred = TRUE))
parallel::stopCluster(cl)
tictoc::toc()

#1772.517 sec elapsed - from talapas - over 16 hours with all tuning parameters
```

Select the best tuning parameters and use them to finalize the workflow.

+ Final parameters chosen (after tuning/selecting best) - 10 neighbors (others are default due to computation time)
```{r}
knn_best <- knn_res %>% 
  select_best(metric = "rmse")

knn_flo_final <- finalize_workflow(knn_flo, knn_best)
```


```{r}
registerDoSEQ() 
knn_final_res <- last_fit(
  knn_flo_final,
  split = d_split)

knn_final_res %>%
  collect_metrics()

```


# Model 3: Lost in the Forest - Random Forest (JP)]

Final parameters chosen (after tuning/selecting best) - mtry = 5, min_n = 40, trees = 1000

13919.058 sec elapsed - from talapas

### Modified recipe to increase predictive performance (JP)

# Comparison

+ Plot comparing performance metrics

+ Explanation of best model

```{r from-rds-files}
#info extracted from rds files & from talapas .Rout files (see lab 4 repo/files for final folder)
#Regression model
#1552.898 sec elapsed on talapas
#RMSE = 89.02

#KNN model
#1772.517 sec elapsed on talapas
#RMSE 92.2 

#RF model
#13919.058 sec elapsed on talapas
#RMSE 85.8
```


+ How to get predictions from test.csv 

```{r predict-new-test-data}
#import
test <- read_csv("data/test.csv",
                 col_types = cols(.default = col_guess(), 
                                  calc_admn_cd = col_character()))

#join
test1 <- test %>% 
  left_join(frl_stu) %>% 
  left_join(staff) %>% 
  left_join(ethnicities)

```

```{r predict-new-test-data-final-fit}
#update with final chosen model (best fit)
#If you want to use your model to predict the response for new observations, you need to use the fit() function on your workflow and the dataset that you want to fit the final model on (e.g. the complete training + testing dataset). 

#workflow
fit_workflow <- fit(FINALIZED_WORKFLOW_OBJECT_NAME, d)#Should this be d_train based on lab 3key. This blog says not - http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/

fit_workflow #view

#use model to make predictions for test dataset (adding test1 as new data)
preds_final <- predict(fit_workflow, test1)

######################
pred_frame <- tibble(Id = test1$id, Predicted = preds_final$.pred)

#write_csv(pred_frame, "FILE-NAME.csv")

```

